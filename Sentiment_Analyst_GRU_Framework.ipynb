{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNIWaI8TuBlx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import os\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow import keras\n",
        "import datetime\n",
        "import h5py\n",
        "import pickle\n",
        "from tensorflow.keras.callbacks import *\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71GUxK5rhSDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "a3c07fd6-7f6d-4b57-aa2d-200d1571037f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv(io.StringIO(uploaded[\"training.1600000.processed.noemoticon.csv\"].decode(\"Latin-1\")))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mTtlbOFuPIu"
      },
      "source": [
        "df = pd.read_csv(io.StringIO(uploaded[\"training.1600000.processed.noemoticon.csv\"].decode(\"Latin-1\")))\n",
        "df['target'] = df[\"target\"].replace(4, 1) # replace labels of 4 with 1, easier for the computer to compute\n",
        "df.drop([\"id\",\"flag\",\"user\"],axis=1,inplace=True) # drop unneeded columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QniRTVxquSo4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5511b33a-50ae-499c-a34d-861c38f8fdb1"
      },
      "source": [
        "# find the amount of distinct words found in the dataset\n",
        "full_text = []\n",
        "for row in df[\"text\"]:\n",
        "    row = row.split()\n",
        "    full_text += row\n",
        "\n",
        "distinct_words = list(dict.fromkeys(full_text))\n",
        "print(\"Found \" + str(len(distinct_words)) + \" unique words\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1350598 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoe86NQLuU6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "825cbe9d-4e74-4dce-9c2a-ce4e7b1b69e9"
      },
      "source": [
        "X = df[\"text\"]\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=len(distinct_words)) \n",
        "tokenizer.fit_on_texts(X) \n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train) \n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeRtKWdguZzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f42972e7-6fcd-4aed-a0af-a0ad56dfe89a"
      },
      "source": [
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=280)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=280)\n",
        "X_test[0].shape\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lePt_VLy2D0"
      },
      "source": [
        "with open('/content/gdrive/My Drive/Colab Notebooks/CheckPoints/LSTM-TwoLayer/tokenizerBruhhhh.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffErXNhPR_GA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79e6ada4-ecd3-4aa3-aea5-7b2e06afa7e2"
      },
      "source": [
        "\n",
        "\n",
        "max_features = len(distinct_words) + 1 # unique vocab\n",
        "maxlen = 280 # the number of words per data point\n",
        "embedding_size = 264 # the dimensions that each word is converted to\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "filepath=\"/content/gdrive/My Drive/Colab Notebooks/CheckPoints/LSTM-TwoLayer/theepochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_accuracy\", verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "callbacks_list.append(tensorboard_callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bclpKOI8ucje",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "49f8d6a7-7996-4522-eeff-c0ee70d10c80"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, embedding_size, input_length=maxlen) )\n",
        "\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "model.add( GRU(264, dropout=0.3, recurrent_dropout=0.3, return_sequences=True) )\n",
        "\n",
        "model.add( GRU(264, dropout=0.3, recurrent_dropout=0.3) ) \n",
        "\n",
        "model.add( Dense(1, activation='sigmoid') )\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'] )\n",
        "\n",
        "fitModel = model.fit(X_train, y_train, epochs = 7, batch_size = 324, validation_data=(X_test, y_test), \n",
        "                     verbose=1, callbacks=callbacks_list) \n",
        "\n",
        "print( model.summary() )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/7\n",
            "3704/3704 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.8090\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.82786, saving model to /content/gdrive/My Drive/Colab Notebooks/CheckPoints/LSTM-TwoLayer/theepochs:001-val_acc:0.828.hdf5\n",
            "3704/3704 [==============================] - 13175s 4s/step - loss: 0.4150 - accuracy: 0.8090 - val_loss: 0.3814 - val_accuracy: 0.8279\n",
            "Epoch 2/7\n",
            "3704/3704 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.8762\n",
            "Epoch 00002: val_accuracy did not improve from 0.82786\n",
            "3704/3704 [==============================] - 12721s 3s/step - loss: 0.2928 - accuracy: 0.8762 - val_loss: 0.4349 - val_accuracy: 0.8082\n",
            "Epoch 3/7\n",
            " 458/3704 [==>...........................] - ETA: 2:50:37 - loss: 0.2207 - accuracy: 0.9092"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}